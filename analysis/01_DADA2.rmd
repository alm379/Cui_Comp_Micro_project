---
title: "Infer ASVs with DADA2"
author: "Andrea Martinez"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document: 
    code_folding: show
    theme: spacelab
    highlight: pygments
    keep_md: no
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
      toc_depth: 3
  keep_md: true  
editor_options: 
  chunk_output_type: console
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center", 
                      fig.path = "../figures/01_DADA2/") # send any figure output to this folder
```

# Before you start

## Set my seed 
```{r set-seed}
# Any number can be chosen 
set.seed(0601)

```


# Goals of this file 

1. Use raw fastq files and generate quality plots to assess quality of reads.
2. Filter and trim out bad sequences and bases from our sequencing files. 
3. Write out fastq files with high quality sequences. 
4. Evaluate the quality from our filter and trim. 
5. Infer Errors on forward and reverse reads individually.
6. Identified ASVs on forward and reverse reads separately, using the error model.  
7. Merge forward and reverse ASVs into "contiguous ASVs".  
8. Generate the ASV count table. (`otu_table` input for phyloseq.).

Outputs of this file: 
1. ASV Count Table: `otu_table`  
2. Taxonomy Table  `tax_table`
3. Sample Information: `sample_data`  track the reads lots throughout DADA2 workflow. 

# Load Libraries 
```{r load-libraries}
# Efficient package loading with pacman 
# Don't forget to install pacman and DT if you don't have it yet. :) 
pacman::p_load(tidyverse, BiocManager, devtools, dada2, 
               phyloseq, patchwork, DT, iNEXT, vegan,
               install = FALSE)
```

#Load data
```{r load-data}
# Set the raw fastq path to the raw sequencing files 
# Path to the fastq files 
raw_fastqs_path <- "data/01_DADA2/raw_fastqs"
raw_fastqs_path
# What files are in this path?
head(list.files(raw_fastqs_path))
# How many files are there? 
str(list.files(raw_fastqs_path))
# Create vector of forward reads
forward_reads <- list.files(raw_fastqs_path, pattern = "_1.fastq.gz", full.names = TRUE)  
# Intuition Check 
head(forward_reads)
# Create a vector of reverse reads 
reverse_reads <- list.files(raw_fastqs_path, pattern = "_2.fastq.gz", full.names = TRUE)
# Intuition Check 
head(reverse_reads)
```

# Assess Raw Read Quality 
## Evaluate raw sequence quality 

Quality of the raw reads *before* trimming

### Plot 12 random samples of plots 
```{r raw-quality-plot, fig.width=12, fig.height=8}
# Randomly select 12 samples from dataset to evaluate 
# Selecting 12 is typically better than 2 (like we did in class for efficiency)
random_samples <- sample(1:length(reverse_reads), size = 12)
random_samples

# Calculate and plot quality of these two samples
forward_filteredQual_plot_12 <- plotQualityProfile(forward_reads[random_samples]) + 
  labs(title = "Forward Read: Raw Quality")
reverse_filteredQual_plot_12 <- plotQualityProfile(reverse_reads[random_samples]) + 
  labs(title = "Reverse Read: Raw Quality")
# Plot them together with patchwork
forward_filteredQual_plot_12 + reverse_filteredQual_plot_12
```

### Aggregated Raw Quality Plots 
```{r raw-aggregate-plot, fig.width=5.5, fig.height=3.5}
# Aggregate all QC plots 
# Forward reads
forward_preQC_plot <- 
  plotQualityProfile(forward_reads, aggregate = TRUE) + 
  labs(title = "Forward Pre-QC")
# reverse reads
reverse_preQC_plot <- 
  plotQualityProfile(reverse_reads, aggregate = TRUE) + 
  labs(title = "Reverse Pre-QC")

preQC_aggregate_plot <- 
  # Plot the forward and reverse together 
  forward_preQC_plot + reverse_preQC_plot

# Show the plot
preQC_aggregate_plot
```

The quality plots show binned quality scores. This binning can have effects on the error-rate learning step. The reads are of generally high quality (Phred score >30) with the exception of base 247 for the forward read of a single sample as confirmed by the MultiQC report (/local/workdir/alm379/git_repos/Cui_Comp_Micro_project/QC/MultiQC_Results/multiqc_report.html). Since this low quality base is only present in one sample, I will not trim it in the filtering step, but will take note of it for downstream analyses.

## Prepare a placeholder for filtered reads 
```{r prep-filtered-sequences}
# vector of our samples, extract sample name from files 
samples <- sapply(strsplit(basename(forward_reads), "_"), `[`,1) 
# Intuition Check 
head(samples)

# Place filtered reads into filtered_fastqs_path
filtered_fastqs_path <- "data/01_DADA2/02_filtered_fastqs"
filtered_fastqs_path

# create 2 variables: filtered_F, filtered_R
filtered_forward_reads <- 
  file.path(filtered_fastqs_path, paste0(samples, "_1_filtered.fastq.gz"))
length(filtered_forward_reads)

# reverse reads
filtered_reverse_reads <- 
  file.path(filtered_fastqs_path, paste0(samples, "_2_filtered.fastq.gz"))
head(filtered_reverse_reads)
```

# Filter and Trim Reads

- `maxEE` is a quality filtering threshold applied to expected errors. Here, if there's 2 expected errors. It's ok. But more than 2. Throw away the sequence. Two values, first is for forward reads; second is for reverse reads.  
- `trimLeft` can be used to remove the beginning bases of a read *(Here used to trim out primers)*
- `truncLen` can be used to trim your sequences after a specific base pair when the quality gets lower. Though, please note that this will shorten the ASVs! For example, this can be used when the quality of the sequence suddenly gets lower, or clearly is typically lower. So, if the quality of the read drops below a phred score of 25 (on the y-axis of the plotQualityProfile above, which indicates ~99.5% confidence per base).  
- `maxN` the number of N bases. Here, using ASVs, we should ALWAYS remove all Ns from the data.  

```{r filter-and-trim}
filtered_reads <- filterAndTrim(forward_reads, filtered_forward_reads,
              reverse_reads, filtered_reverse_reads,
              trimLeft = c(19,20),
              maxN = 0, maxEE = c(2,2), truncQ = 2, 
              rm.phix = TRUE, compress = TRUE, 
               multithread = 5)
```

# Assess Trimmed Read Quality 

```{r filterTrim-quality-plots,  fig.width=12, fig.height=8}
# Plot the 12 random samples after QC
forward_filteredQual_plot_12 <- 
  plotQualityProfile(filtered_forward_reads[random_samples]) + 
  labs(title = "Trimmed Forward Read Quality")

reverse_filteredQual_plot_12 <- 
  plotQualityProfile(filtered_reverse_reads[random_samples]) + 
  labs(title = "Trimmed Reverse Read Quality")

# Put the two plots together 
forward_filteredQual_plot_12 + reverse_filteredQual_plot_12
```

## Aggregated Trimmed Plots 
```{r qc-aggregate-plot, fig.width=5.5, fig.height=3.5}
# Aggregate all QC plots 
# Forward reads
forward_postQC_plot <- 
  plotQualityProfile(filtered_forward_reads, aggregate = TRUE) + 
  labs(title = "Forward Post-QC")

# reverse reads
reverse_postQC_plot <- 
  plotQualityProfile(filtered_reverse_reads, aggregate = TRUE) + 
  labs(title = "Reverse Post-QC")

postQC_aggregate_plot <- 
  # Plot the forward and reverse together 
  forward_postQC_plot + reverse_postQC_plot

# Show the plot
postQC_aggregate_plot
```

The trimmed reads are of even higher quality than the raw reads, as expected. All Phred scores are above 30 and even before calculating read retention, it seems a fair number of reads were retained. 

## Stats on read output from `filterAndTrim`

```{r filterTrim-stats}
# Make output into dataframe 
filtered_df <- as.data.frame(filtered_reads)
head(filtered_df)


# calculate some stats 
filtered_df %>%
  reframe(median_reads_in = median(reads.in),
          median_reads_out = median(reads.out),
          median_percent_retained = (median(reads.out)/median(reads.in)))
```

86% of reads were retained though filtering. This seems like an acceptable retention level. Initially, I used trimRight = 3 to trim base 247 before noticing that the base quality was low only for one read. Surprisingly, this led to a slightly higher retention of 86.33% which I would not expect with an additional parameter which I interpreted as making the filtering step more stringent. I ultimately decided to use the command that did not include the trimRight parameter. Up until this point, the binned quality scores generated by the Illumina Novaseq platform do not seem incompatible with this workflow. However, the error modeling step will need to account for the binned quality scores in order to use the estimated error rates for inferring ASVs. When the first error estimation model was used for inferring ASVs, only 16 ASVs were detected.


## Learn the errors 
# First Attempt
```{r learn-errors, fig.width=12, fig.height=8}
# Forward reads 
#error_forward_reads <- 
#  learnErrors(filtered_forward_reads, multithread = 5)
# Plot Forward  
#forward_error_plot <- 
#  plotErrors(error_forward_reads, nominalQ = TRUE) + 
#  labs(title = "Forward Read Error Model")

# Reverse reads 
#error_reverse_reads <- 
#  learnErrors(filtered_reverse_reads, multithread = 5)
# Plot reverse
#reverse_error_plot <- 
#  plotErrors(error_reverse_reads, nominalQ = TRUE) + 
#  labs(title = "Reverse Read Error Model")

# Put the two plots together
#forward_error_plot + reverse_error_plot
```

The DADA2 learnErrors command was found to be incompatible for data with binned quality scores. I consulted a github forum discussing the issue (https://github.com/benjjneb/dada2/issues/1307). Proposed solutions included using the loessErrfun function enforcing monotonicity. I used a code shared by JacobRPrice from this forum.

# Second Attempt

```{r learn-errors-2}
library(magrittr)
library(dplyr)

loessErrfun_mod <- function(trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow=0, ncol=length(qq))
  for(nti in c("A","C","G","T")) {
    for(ntj in c("A","C","G","T")) {
      if(nti != ntj) {
        errs <- trans[paste0(nti,"2",ntj),]
        tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),])
        rlogp <- log10((errs+1)/tot)  # 1 psuedocount for each err, but if tot=0 will give NA
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp)

        # original
        # ###! mod.lo <- loess(rlogp ~ q, df, weights=errs) ###!
        # mod.lo <- loess(rlogp ~ q, df, weights=tot) ###!
        # #        mod.lo <- loess(rlogp ~ q, df)

        # Gulliem Salazar's solution
        # https://github.com/benjjneb/dada2/issues/938
        # mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),span = 2)

        # only change the weights
        mod.lo <- loess(rlogp ~ q, df, weights = log10(tot))

        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred)>maxrli] <- pred[[maxrli]]
        pred[seq_along(pred)<minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      } # if(nti != ntj)
    } # for(ntj in c("A","C","G","T"))
  } # for(nti in c("A","C","G","T"))

  # HACKY
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-7
  est[est>MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est<MIN_ERROR_RATE] <- MIN_ERROR_RATE

  # enforce monotonicity
  # https://github.com/benjjneb/dada2/issues/791
  estorig <- est
  est <- est %>%
    data.frame() %>%
    mutate_all(funs(case_when(. < X40 ~ X40,
                              . >= X40 ~ .))) %>% as.matrix()
  rownames(est) <- rownames(estorig)
  colnames(est) <- colnames(estorig)

  # Expand the err matrix with the self-transition probs
  err <- rbind(1-colSums(est[1:3,]), est[1:3,],
               est[4,], 1-colSums(est[4:6,]), est[5:6,],
               est[7:8,], 1-colSums(est[7:9,]), est[9,],
               est[10:12,], 1-colSums(est[10:12,]))
  rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T"))
  colnames(err) <- colnames(trans)
  # Return
  return(err)
}

# check what this looks like
errF <- learnErrors(filtered_forward_reads
  ,
  multithread = 5,
  errorEstimationFunction = loessErrfun_mod,
  verbose = TRUE
)
errR <- learnErrors(filtered_reverse_reads
  ,
  multithread = 5,
  errorEstimationFunction = loessErrfun_mod,
  verbose = TRUE
)
dada2:::checkConvergence(errR)

forward_error_plot <- 
 plotErrors(errF, nominalQ = TRUE) + 
 labs(title = "Forward Read Error Model")

reverse_error_plot <- 
 plotErrors(errR, nominalQ = TRUE) + 
 labs(title = "Reverse Read Error Model")
forward_error_plot + reverse_error_plot

# Success!
```

This error model looks much more reasonable than the prior model which had "dips" in the estimated error rates caused by binned quality scores. As expected, error rates decrease as quality scores increase. These error estimates can now be applied to the DADA function for inferring ASVs.

# Infer ASVs 
```{r infer-ASVs}
#Infer ASVs on the forward sequences
dada_forward <- dada(filtered_forward_reads,
                     err = errF, 
                     multithread = 5)
# Inspect 
dada_forward[1]
dada_forward[30]

dada_reverse <- dada(filtered_reverse_reads,
                     err = errR,
                     multithread = 5)
# Inspect 
dada_reverse[1]
dada_reverse[30]
```
# Merge Forward & Reverse ASVs

Now, merge the forward and reverse ASVs into contigs. 

```{r merge-ASVs}
# merge forward and reverse ASVs
merged_ASVs <- mergePairs(dada_forward, filtered_forward_reads, 
                          dada_reverse, filtered_reverse_reads,
                          verbose = TRUE)
# Evaluate the output 
typeof(merged_ASVs)
length(merged_ASVs)
names(merged_ASVs)

# Inspect the merger data.frame from the 20210602-MA-ABB1P 
head(merged_ASVs[[3]])
```

# Create Raw ASV Count Table 
```{r generate-ASV-table, fig.width=3.5, fig.height=3}
# Create the ASV Count Table 
raw_ASV_table <- makeSequenceTable(merged_ASVs)
# Write out the file to data/01_DADA2


# Check the type and dimensions of the data
dim(raw_ASV_table)
class(raw_ASV_table)
typeof(raw_ASV_table)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table)))

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Raw distribution of ASV length")
###################################################
###################################################
# TRIM THE ASVS
# Trim the ASVs to be the right size for the V3-V4 region. The raw ASV table and plot shows most reads falling between a length of 400-433 with 2 major peaks around 423 bp. and 431 bp.

raw_ASV_table_trimmed <- raw_ASV_table[,nchar(colnames(raw_ASV_table)) %in% 400:433]
# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table_trimmed)))

# What proportion is left of the sequences? 
sum(raw_ASV_table_trimmed)/sum(raw_ASV_table)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length")

# Let's zoom in on the plot 
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length") + 
  scale_y_continuous(limits = c(0, 500))

# Decision to keep this trim length after inspecting the zoomed in graph in order to keep the ~50 reads at around 400 bp in length.
```

# Remove Chimeras
```{r rm_chimeras, fig.width=3.5, fig.height=3}
# Remove the chimeras in the raw ASV table
noChimeras_ASV_table <- removeBimeraDenovo(raw_ASV_table_trimmed, 
                                           method="consensus", 
                                           multithread=5, verbose=TRUE)

# Check the dimensions
dim(noChimeras_ASV_table)

# What proportion is left of the sequences? 
sum(noChimeras_ASV_table)/sum(raw_ASV_table_trimmed)
sum(noChimeras_ASV_table)/sum(raw_ASV_table)

# Plot it 
data.frame(Seq_Length_NoChim = nchar(getSequences(noChimeras_ASV_table))) %>%
  ggplot(aes(x = Seq_Length_NoChim )) + 
  geom_histogram()+ 
  labs(title = "Trimmed + Chimera Removal distribution of ASV length")


```

# Track the read counts
```{r track_reads, fig.width=6, fig.height=4}
# Function to identify number seqs 
getN <- function(x) sum(getUniques(x))

# Make the table to track the seqs 
track <- cbind(filtered_reads, 
               sapply(dada_forward, getN),
               sapply(dada_reverse, getN),
               sapply(merged_ASVs, getN),
               rowSums(noChimeras_ASV_table))
head(track)

# Update column names to be more informative (most are missing at the moment!)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track) <- samples

# Generate a dataframe to track the reads through our DADA2 pipeline
track_counts_df <- 
  track %>%
  # make it a dataframe
  as.data.frame() %>%
  rownames_to_column(var = "names") %>%
  mutate(perc_reads_retained = 100 * nochim / input)

# Visualize it in table format 
DT::datatable(track_counts_df)
# Plot it!
track_counts_df %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type)) + 
  geom_line(aes(group = names), color = "grey") + 
  geom_point(shape = 21, size = 3, alpha = 0.8) + 
  scale_fill_brewer(palette = "Spectral") + 
  labs(x = "Filtering Step", y = "Number of Sequences") + 
  theme_bw()
```

# Assign Taxonomy 

Here, we will use the Silva database version 138! 
This differs from the workflow of the authors who own this data, who used The Greengenes database Release 13.8.

```{r assign-tax}
# Classify the ASVs against a reference set using the RDP Naive Bayesian Classifier described by Wang et al., (2007) in AEM
taxa_train <- 
  assignTaxonomy(noChimeras_ASV_table, 
                 "/workdir/in_class_data/taxonomy/silva_nr99_v138.1_train_set.fa.gz", 
                 multithread=5)

# Add the genus/species information 
taxa_addSpecies <- 
  addSpecies(taxa_train, 
             "/workdir/in_class_data/taxonomy/silva_species_assignment_v138.1.fa.gz")

# Inspect the taxonomy 
taxa_print <- taxa_addSpecies # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
#View(taxa_print)

# Inspect the taxonomy 
taxa_print <- taxa_addSpecies # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
#View(taxa_print)
```
# Prepare the data for export! 

## 1. ASV Table 
Below, we will prepare the following: 

1. Two ASV Count tables: 
      a. With ASV seqs: ASV headers include the *entire* ASV sequence.
      b. with ASV names: This includes re-written and shortened headers like ASV_1, ASV_2, etc, which will match the names in our fasta file below.  
2. `ASV_fastas`: A fasta file that we can use to build a tree for phylogenetic analyses (e.g. phylogenetic alpha diversity metrics or UNIFRAC dissimilarty).  

### Finalize ASV Count Tables 
```{r prepare-ASVcount-table}
########### 2. COUNT TABLE ###############
############## Modify the ASV names and then save a fasta file!  ############## 
# Give headers more manageable names
# First pull the ASV sequences
asv_seqs <- colnames(noChimeras_ASV_table)
asv_seqs[1:5]
# make headers for our ASV seq fasta file, which will be our asv names
asv_headers <- vector(dim(noChimeras_ASV_table)[2], mode = "character")
asv_headers[1:5]

# loop through vector and fill it in with ASV names 
for (i in 1:dim(noChimeras_ASV_table)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep = "_")
}

# intitution check
asv_headers[1:5]

##### Rename ASVs in table then write out our ASV fasta file! 
#View(noChimeras_ASV_table)
asv_tab <- t(noChimeras_ASV_table)
#View(asv_tab)

## Rename our asvs! 
row.names(asv_tab) <- sub(">", "", asv_headers)
#View(asv_tab)
```


## 2. Taxonomy Table 
```{r prepare-tax-table}
# Inspect the taxonomy table
#View(taxa_addSpecies)

##### Prepare tax table 
# Add the ASV sequences from the rownames to a column 
new_tax_tab <- 
  taxa_addSpecies%>%
  as.data.frame() %>%
  rownames_to_column(var = "ASVseqs") 
head(new_tax_tab)

# intution check 
stopifnot(new_tax_tab$ASVseqs == colnames(noChimeras_ASV_table))

# Now let's add the ASV names 
rownames(new_tax_tab) <- rownames(asv_tab)
head(new_tax_tab)

### Final prep of tax table. Add new column with ASV names 
asv_tax <- 
  new_tax_tab %>%
  # add rownames from count table for phyloseq handoff
  mutate(ASV = rownames(asv_tab)) %>%
  # Resort the columns with select
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, Species, ASV, ASVseqs)

head(asv_tax)

# Intution check
stopifnot(asv_tax$ASV == rownames(asv_tax), rownames(asv_tax) == rownames(asv_tab))
```

# Write `01_DADA2` files

Now, we will write the files! We will write the following to the `data/01_DADA2/` folder. We will save both as files that could be submitted as supplements AND as .RData objects for easy loading into the next steps into R.:  

1. `ASV_counts.tsv`: ASV count table that has ASV names that are re-written and shortened headers like ASV_1, ASV_2, etc, which will match the names in our fasta file below. This will also be saved as `data/01_DADA2/ASV_counts.RData`.
2. `ASV_counts_withSeqNames.tsv`: This is generated with the data object in this file known as `noChimeras_ASV_table`. ASV headers include the *entire* ASV sequence ~250bps.  In addition, we will save this as a .RData object as `data/01_DADA2/noChimeras_ASV_table.RData` as we will use this data in `analysis/02_Taxonomic_Assignment.Rmd` to assign the taxonomy from the sequence headers.  
3. `ASVs.fasta`: A fasta file output of the ASV names from `ASV_counts.tsv` and the sequences from the ASVs in `ASV_counts_withSeqNames.tsv`. A fasta file that we can use to build a tree for phylogenetic analyses (e.g. phylogenetic alpha diversity metrics or UNIFRAC dissimilarty).  
4. We will also make a copy of `ASVs.fasta` in `data/02_TaxAss_FreshTrain/` to be used for the taxonomy classification in the next step in the workflow.  
5. Write out the taxonomy table
6. `track_read_counts.RData`: To track how many reads we lost throughout our workflow that could be used and plotted later. We will add this to the metadata in `analysis/02_Taxonomic_Assignment.Rmd`.   


```{r save-files}
# Save output as regular files
# Save to regular .tsv file 
# Write BOTH the modified and unmodified ASV tables to a file!
# Write count table with ASV numbered names (e.g. ASV_1, ASV_2, etc)
write.table(asv_tab, "data/01_DADA2/ASV_counts.tsv", sep = "\t", quote = FALSE, col.names = NA)
# Write count table with ASV sequence names
write.table(noChimeras_ASV_table, "data/01_DADA2/ASV_counts_withSeqNames.tsv", sep = "\t", quote = FALSE, col.names = NA)
# Write out the fasta file for reference later on for what seq matches what ASV
asv_fasta <- c(rbind(asv_headers, asv_seqs))
# Save to a file!
write(asv_fasta, "data/01_DADA2/ASVs.fasta")


#Save the taxonomy tables 
# Write the table 
write.table(asv_tax, "data/01_DADA2/ASV_taxonomy.tsv", sep = "\t", quote = FALSE, col.names = NA)


# THIRD, let's save to a RData object 
# Each of these files will be used in the analysis/02_Taxonomic_Assignment
# RData objects are for easy loading :) 
save(noChimeras_ASV_table, file = "data/01_DADA2/noChimeras_ASV_table.RData")
save(asv_tab, file = "data/01_DADA2/ASV_counts.RData")
# And save the track_counts_df a R object, which we will merge with metadata information in the next step of the analysis in nalysis/02_Taxonomic_Assignment. 
save(track_counts_df, file = "data/01_DADA2/track_read_counts.RData")
```

# Session Information 
```{r session-info}
# Ensure reproducibility 
devtools::session_info()
```